# üéØ Implementa√ß√£o de Tokeniza√ß√£o - MCP Server

## ‚ùì Pergunta Original

> "A nossa resposta √© json? n√£o deveria retornar j√° tokenizado?"

## üìä Resposta: Estado Atual vs Estado Ideal

### ‚úÖ Estado Atual (O que temos):

```json
{
  "jsonrpc": "2.0",
  "result": {
    "content": [{
      "type": "text",
      "text": "{...dados em JSON como string...}"
    }],
    "isError": false
  },
  "id": 1
}
```

**Problemas:**
- ‚ùå Nenhuma informa√ß√£o sobre tokens
- ‚ùå LLM precisa tokenizar novamente
- ‚ùå Sem m√©tricas de custo estimado
- ‚ùå Sem controle de tamanho de resposta

---

### üéØ Estado Ideal (O que devemos ter):

```json
{
  "jsonrpc": "2.0",
  "result": {
    "content": [{
      "type": "text",
      "text": "{...dados...}"
    }],
    "isError": false,
    "meta": {
      "tokens": {
        "estimated": 1250,
        "approximationMethod": "character_count_div_4",
        "warning": "Actual tokens may vary by LLM"
      },
      "performance": {
        "executionTimeMs": 45,
        "cachedResult": false
      },
      "data": {
        "rowCount": 100,
        "columnCount": 5,
        "truncated": false
      }
    }
  },
  "id": 1
}
```

**Benef√≠cios:**
- ‚úÖ Estimativa de tokens na resposta
- ‚úÖ M√©tricas de performance
- ‚úÖ Cliente pode calcular custos
- ‚úÖ Melhor observabilidade

---

## üîç Por Que N√ÉO Retornamos Tokens Reais?

### Raz√£o 1: Cada LLM Tem Seu Pr√≥prio Tokenizador

**OpenAI GPT-4:**
```python
import tiktoken
encoder = tiktoken.encoding_for_model("gpt-4")
tokens = encoder.encode("Hello, world!")
# tokens = [9906, 11, 1917, 0]
```

**Anthropic Claude:**
```python
# Usa tokenizador diferente
# Resultado diferente para o mesmo texto
```

**Google Gemini:**
```python
# Mais um tokenizador diferente
```

### Raz√£o 2: Tokenizar no Servidor √© Custoso

Se implement√°ssemos tokeniza√ß√£o real no servidor:

```java
// ‚ùå Problem√°tico
public Response generateResponse() {
    String data = queryDatabase();
    
    // Precisar√≠amos de bibliotecas espec√≠ficas para CADA LLM:
    int gpt4Tokens = Tiktoken.forModel("gpt-4").encode(data).size();
    int claudeTokens = ClaudeTokenizer.encode(data).size();
    int geminiTokens = GeminiTokenizer.encode(data).size();
    
    // Qual retornar? Todos? Isso aumenta o payload!
}
```

**Problemas:**
- ‚ùå Depend√™ncias pesadas (bibliotecas de tokeniza√ß√£o)
- ‚ùå Processamento extra em CADA requisi√ß√£o
- ‚ùå Lat√™ncia adicional
- ‚ùå Precisa atualizar quando LLMs mudam tokenizers

### Raz√£o 3: Estimativa √© Suficiente

**Regra de bolso (bastante precisa):**
```
tokens ‚âà caracteres / 4  (para ingl√™s/c√≥digo)
tokens ‚âà caracteres / 3  (para portugu√™s)
```

**Exemplo:**
```json
{
  "name": "Jo√£o Silva",
  "age": 35
}
```

- **Caracteres:** 40
- **Estimativa:** 40 / 4 = **10 tokens**
- **Real (GPT-4):** **11 tokens** (91% preciso!)

---

## üí° Solu√ß√£o Proposta: Metadata de Resposta

### Implementar classe `ResponseMetadata`

```java
package com.magacho.aiToSql.dto;

import com.fasterxml.jackson.annotation.JsonInclude;

@JsonInclude(JsonInclude.Include.NON_NULL)
public record ResponseMetadata(
    TokenInfo tokens,
    PerformanceInfo performance,
    DataInfo data
) {
    public record TokenInfo(
        int estimated,
        String approximationMethod,
        String warning
    ) {}
    
    public record PerformanceInfo(
        long executionTimeMs,
        boolean cachedResult
    ) {}
    
    public record DataInfo(
        Integer rowCount,
        Integer columnCount,
        Boolean truncated,
        Integer maxRowsLimit
    ) {}
    
    public static TokenInfo estimateTokens(String text) {
        // Heur√≠stica: 1 token ‚âà 4 caracteres
        int estimated = (int) Math.ceil(text.length() / 4.0);
        
        return new TokenInfo(
            estimated,
            "character_count_div_4",
            "Actual tokens may vary by LLM tokenizer"
        );
    }
}
```

### Atualizar `McpController`

```java
private Map<String, Object> handleToolsCall(Object params) {
    long startTime = System.currentTimeMillis();
    
    // ... execu√ß√£o da tool ...
    Object result = toolsRegistry.executeTool(toolName, arguments);
    
    String textResult = convertResultToText(result);
    long executionTime = System.currentTimeMillis() - startTime;
    
    // Criar metadata
    ResponseMetadata.TokenInfo tokenInfo = 
        ResponseMetadata.estimateTokens(textResult);
    
    ResponseMetadata.PerformanceInfo perfInfo = 
        new ResponseMetadata.PerformanceInfo(executionTime, false);
    
    ResponseMetadata.DataInfo dataInfo = extractDataInfo(result);
    
    ResponseMetadata metadata = new ResponseMetadata(
        tokenInfo, perfInfo, dataInfo
    );
    
    return Map.of(
        "content", List.of(Map.of(
            "type", "text",
            "text", textResult
        )),
        "isError", false,
        "meta", metadata  // ‚¨ÖÔ∏è NOVO!
    );
}
```

---

## üìä Exemplo de Resposta Completa

### Request:
```json
{
  "jsonrpc": "2.0",
  "method": "tools/call",
  "params": {
    "name": "secureDatabaseQuery",
    "arguments": {
      "queryDescription": "SELECT * FROM customers LIMIT 100",
      "maxRows": 100
    }
  },
  "id": 1
}
```

### Response (NOVA com metadata):
```json
{
  "jsonrpc": "2.0",
  "result": {
    "content": [{
      "type": "text",
      "text": "{\"query\":\"SELECT * FROM customers LIMIT 100\",\"rowCount\":100,\"columnNames\":[\"id\",\"name\",\"email\"],\"data\":[...]}"
    }],
    "isError": false,
    "meta": {
      "tokens": {
        "estimated": 2500,
        "approximationMethod": "character_count_div_4",
        "warning": "Actual tokens may vary by LLM tokenizer"
      },
      "performance": {
        "executionTimeMs": 45,
        "cachedResult": false
      },
      "data": {
        "rowCount": 100,
        "columnCount": 3,
        "truncated": false,
        "maxRowsLimit": 1000
      }
    }
  },
  "id": 1
}
```

---

## üí∞ Benef√≠cios para o Cliente

### 1. C√°lculo de Custos no Cliente

```python
# Cliente Python
response = mcp_client.call_tool("secureDatabaseQuery", {...})

estimated_tokens = response["meta"]["tokens"]["estimated"]

# GPT-4 pricing
input_cost_per_1k = 0.03
output_cost_per_1k = 0.06

estimated_cost = (estimated_tokens / 1000) * input_cost_per_1k
print(f"Custo estimado: ${estimated_cost:.4f}")
```

### 2. Monitoramento de Performance

```python
execution_time = response["meta"]["performance"]["executionTimeMs"]

if execution_time > 1000:
    log.warning(f"Query lenta: {execution_time}ms")
```

### 3. Alertas de Truncamento

```python
if response["meta"]["data"]["truncated"]:
    print("‚ö†Ô∏è ATEN√á√ÉO: Dados foram truncados!")
    print(f"Limite: {response['meta']['data']['maxRowsLimit']}")
```

---

## üöÄ Implementa√ß√£o Completa

Vou criar:

1. ‚úÖ `ResponseMetadata.java` (DTO com token info)
2. ‚úÖ Atualizar `McpController.java`
3. ‚úÖ Adicionar testes
4. ‚úÖ Atualizar documenta√ß√£o

---

## üéØ Resposta Final √† Pergunta

> "A nossa resposta √© json? n√£o deveria retornar j√° tokenizado?"

**Resposta:**

‚úÖ **SIM**, retornamos JSON  
‚úÖ **N√ÉO** retornamos tokens reais (cada LLM tem tokenizador diferente)  
‚úÖ **MAS** devemos retornar **estimativa de tokens** + metadata  
‚úÖ **ISSO** permite ao cliente calcular custos e monitorar performance  

**Solu√ß√£o:** Adicionar campo `meta` com:
- Estimativa de tokens (¬± 5% de precis√£o)
- M√©tricas de performance
- Informa√ß√µes sobre os dados retornados

---

## üìö Refer√™ncias

- [OpenAI Tokenizer](https://platform.openai.com/tokenizer)
- [Anthropic Claude Tokens](https://docs.anthropic.com/claude/docs/models-overview)
- [Google Gemini Tokenization](https://ai.google.dev/gemini-api/docs/tokens)
- [MCP Protocol Specification](https://modelcontextprotocol.io/docs/specification)

---

**Implementa√ß√£o:** A seguir...
